[
  {
    "id": "238285053869140992",
    "type": "news",
    "url": "https://www.reddit.com/r/artificial/comments/1qmm0vd/bbc_reports_that_chinese_open_models_continue_to/",
    "title": "BBC报道：中国开源模型持续稳步取代美国公司的闭源产品",
    "description": "BBC报道称，中国开源AI模型正稳步取代美国闭源产品，显示开源生态竞争力。",
    "published_date": "2026-01-25T15:27:04.365Z",
    "authors": "",
    "source": "newest submissions : artificial",
    "score": 65,
    "category": "AI资讯",
    "tags": [
      "开源模型",
      "中美AI竞争",
      "闭源产品",
      "AI生态",
      "行业趋势"
    ],
    "details": {
      "content_html": "<table> <tbody><tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1qmm0vd/bbc_reports_that_chinese_open_models_continue_to/\" target=\"_blank\"> <img src=\"https://external-preview.redd.it/8sZTwTmmAoS1dAUajjCGGPMs91LdwnGVa9fICbWTbtI.jpeg?width=640&#x26;crop=smart&#x26;auto=webp&#x26;s=63dc7b4d6780e703937d6abf48a7fd3988ce56aa\" alt=\"BBC reports that Chinese open models continue to steadily muscle out closed offering from US companies\" title=\"BBC reports that Chinese open models continue to steadily muscle out closed offering from US companies\"> </a> </td><td>   submitted by   <a href=\"https://www.reddit.com/user/fattyfoods\" target=\"_blank\"> /u/fattyfoods </a> <br> <span><a href=\"https://www.bbc.com/news/articles/c86v52gv726o\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/artificial/comments/1qmm0vd/bbc_reports_that_chinese_open_models_continue_to/\" target=\"_blank\">[comments]</a></span> </td></tr></tbody></table>"
    }
  },
  {
    "id": "238315674671857665",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qml8ht/d_error_in_sigir_published_paper/",
    "title": "[讨论] SIGIR发表论文中的错误",
    "description": "作者指出SIGIR会议论文将600M参数的BGE-M3误标为100M小模型，质疑同行评审质量。",
    "published_date": "2026-01-25T14:57:08.716Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 45,
    "category": "AI资讯",
    "tags": [
      "学术论文错误",
      "同行评审质量",
      "BGE-M3模型",
      "SIGIR会议",
      "参数规模争议"
    ],
    "details": {
      "content_html": "<div><p>I am just wondering the review quality of SIGIR.</p> <p>I was reading this paper and I found an obvious error.</p> <p>This paper says BGE-M3 is a small model with 100M parameters???</p> <p>This is not a trivial typo since in RQ2.1, they further emphasize it is a small model.</p> <p>However, BGE-M3 has almost 600M parameters (source: <a href=\"https://bge-model.com/bge/bge%5C_m3.html\" target=\"_blank\">https://bge-model.com/bge/bge\\_m3.html</a>)</p> <p>How could the authors, reviewers, chairs not notice this??? The authors are from a well-known group in IR.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/LouisAckerman\" target=\"_blank\"> /u/LouisAckerman </a> <br> <span><a href=\"https://dl.acm.org/doi/pdf/10.1145/3726302.3730285\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qml8ht/d_error_in_sigir_published_paper/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238315674671857666",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qmjzjd/p_understanding_multihead_latent_attention_mla/",
    "title": "理解多头潜在注意力机制（MLA）",
    "description": "深入解析DeepSeek的多头潜在注意力机制：从直觉、数学推导到MHA→GQA→MQA→MLA的演进，包含PyTorch代码及KV缓存融合优化。",
    "published_date": "2026-01-25T14:06:27.105Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 68,
    "category": "AI模型",
    "tags": [
      "多头潜在注意力",
      "注意力机制",
      "KV缓存优化",
      "DeepSeek",
      "PyTorch实现"
    ],
    "details": {
      "content_html": "<div><p>A short deep-dive on Multi-Head Latent Attention (MLA) (from DeepSeek): intuition + math, then a walk from MHA → GQA → MQA → MLA, with PyTorch code and the fusion/absorption optimizations for KV-cache efficiency.</p> <p><a href=\"http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/\" target=\"_blank\">http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/</a></p> </div>   submitted by   <a href=\"https://www.reddit.com/user/shreyansh26\" target=\"_blank\"> /u/shreyansh26 </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmjzjd/p_understanding_multihead_latent_attention_mla/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmjzjd/p_understanding_multihead_latent_attention_mla/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238235518352030720",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/",
    "title": "[讨论] ICML新政策：审稿人将由元审稿人评审。好政策吗？",
    "description": "讨论ICML会议引入元审稿人评审审稿人的新政策及其影响。",
    "published_date": "2026-01-25T12:41:18.447Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 45,
    "category": "AI资讯",
    "tags": [
      "ICML",
      "学术评审",
      "元审稿",
      "会议政策",
      "机器学习社区"
    ],
    "details": {
      "content_html": "<table> <tbody><tr><td> <a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/\" target=\"_blank\"> <img src=\"https://preview.redd.it/my5s96wpqhfg1.png?width=640&#x26;crop=smart&#x26;auto=webp&#x26;s=e5186c9dc3eb008fb9354f7c89ddd8f3c7bad157\" alt=\"[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy?\" title=\"[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy?\"> </a> </td><td>   submitted by   <a href=\"https://www.reddit.com/user/Striking-Warning9533\" target=\"_blank\"> /u/Striking-Warning9533 </a> <br> <span><a href=\"https://i.redd.it/my5s96wpqhfg1.png\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/\" target=\"_blank\">[comments]</a></span> </td></tr></tbody></table>"
    }
  },
  {
    "id": "238235518352030721",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/",
    "title": "[讨论] ICML 2026 - 我的论文被ICML直接拒稿，却保留我作为审稿人。哇？",
    "description": "作者论文被ICML会议直接拒稿，却被继续委任为审稿人，引发对学术评审制度的讽刺性反思。",
    "published_date": "2026-01-25T12:34:10.796Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 35,
    "category": "AI资讯",
    "tags": [
      "ICML会议",
      "论文评审",
      "学术制度",
      "审稿人",
      "机器学习社区"
    ],
    "details": {
      "content_html": "<div><p>As the title says, I admire the sheer audacity of the ICML committee. My paper gets desk-rejected, so technically I’m not part of the conference… and yet they’ve assigned me as a continued reviewer. Truly inspiring.</p> <p>Rejected as an author, retained as unpaid labor. Academia really said: you don’t belong here, but your service does.</p> <p>At this point, I assume my role is to review LLM-generated papers and reflect on my life choices.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/ParticularWork8424\" target=\"_blank\"> /u/ParticularWork8424 </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238235518352030722",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qmg4t3/d_ai4pdes_sciml_foundational_models_where_are_we/",
    "title": "[讨论] AI4PDEs、科学机器学习与基础模型：我们将走向何方？",
    "description": "作者探讨AI4PDEs和科学机器学习的发展方向，列举强化学习、神经算子、基础模型等应用，并询问该领域趋势及与机器人学习的关联。",
    "published_date": "2026-01-25T10:54:09.484Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 65,
    "category": "AI资讯",
    "tags": [
      "AI4PDEs",
      "科学机器学习",
      "基础模型",
      "神经算子",
      "机器人学习"
    ],
    "details": {
      "content_html": "<div><p>I'm no ML expert, but a master's student working on computational mechanics, PDEs and some deep learning for these topics. </p> <p>I have been following some groups, papers and trends and it is still unclear what is the exact direction in which AI4PDEs and scientific ML is going into. </p> <p>Recent works show reinforcement learning for fluid dynamics, neural operators applied to irregular domains via transformers, GNNs or PointNet, nice works on diffusion or flow matching for inverse problems with physical constraints, and of course protein ans drug discovery tasks. </p> <p>Robotics folks also are using physics environments for policy learning, which based on my limited knowledge, also include some aspects of scientific machine learning. Of course due to ODEs/PDEs, the field also naturally extends to control theory and chaotic systems. </p> <p>Very recently some groups also published foundational models for PDEs. In robotics, major work on foundation VLA-type models is also going on. </p> <p>Some simulation software providers have also included ML or AI surrogates in their workflows. Agents that can automate complex simulation workflows, ML models that can learn from an existing DoE, and geometric deep learning is applied to iterate designs efficiently on irregular domains. </p> <p><strong>My question</strong>: The research still seems scattered and I am unable to notice any trend. Is this true? Or am I missing a major trend that is picking up in research labs. </p> <p>For e.g. LLMs have had some noticeable trends: initially starting with prompt engineering, then reasoning and logical capabilities, now key focus on agentic systems and so on. </p> <p><strong>Another question I have is</strong>: Is robot learning also aiming to include some aspects of scientific ML, possibly to reduce the sim-to-real gap? </p> <p>I'd like to know opinions and observations from folks interested in these areas. </p> <p>Thank you for the discussion.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Mundane_Chemist3457\" target=\"_blank\"> /u/Mundane_Chemist3457 </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmg4t3/d_ai4pdes_sciml_foundational_models_where_are_we/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmg4t3/d_ai4pdes_sciml_foundational_models_where_are_we/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238235518352030723",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qmftku/d_deepdanbooru_v3_pytorch_port_constant_05_or_0/",
    "title": "[讨论] DeepDanbooru v3 PyTorch移植：加载权重后输出恒为0.5或0",
    "description": "将DeepDanbooru v3移植到PyTorch后，模型对所有标签输出恒为0.5。问题追踪至批归一化层的'running_var'值导致崩溃，涉及Keras/TensorFlow权重转换至PyTorch的已知问题。",
    "published_date": "2026-01-25T10:36:12.147Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 45,
    "category": "AI开发",
    "tags": [
      "模型移植",
      "PyTorch",
      "权重转换",
      "批归一化",
      "DeepDanbooru"
    ],
    "details": {
      "content_html": "<div><p>I'm porting DeepDanbooru v3 (Janouch port) to PyTorch. After mapping 209 layers from Safetensors, the model outputs exactly 0.5 for all tags. I've tracked it back to the Batch Normalization layers. It seems like the 'running_var' values are causing a collapse. Is this a known issue when converting Keras/TensorFlow weights to PyTorch for ResNet architectures? Should I manually initialize the BN stats?</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/RevolutionaryAge70\" target=\"_blank\"> /u/RevolutionaryAge70 </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmftku/d_deepdanbooru_v3_pytorch_port_constant_05_or_0/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qmftku/d_deepdanbooru_v3_pytorch_port_constant_05_or_0/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238201582330134528",
    "type": "news",
    "url": "https://www.reddit.com/r/artificial/comments/1qmdhbb/new_ucla_ai_tool_targets_alzheimers_cases_often/",
    "title": "UCLA新型AI工具瞄准早期诊断中常被漏诊的阿尔茨海默病例",
    "description": "UCLA开发AI工具，旨在提升阿尔茨海默病早期诊断准确率，识别易被漏诊病例。",
    "published_date": "2026-01-25T08:17:38.725Z",
    "authors": "",
    "source": "newest submissions : artificial",
    "score": 78,
    "category": "AI产品",
    "tags": [
      "阿尔茨海默病",
      "早期诊断",
      "医疗AI",
      "UCLA",
      "辅助诊断工具"
    ],
    "details": {
      "content_html": "<table> <tbody><tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1qmdhbb/new_ucla_ai_tool_targets_alzheimers_cases_often/\" target=\"_blank\"> <img src=\"https://external-preview.redd.it/js5qs-2Nbc8exH7cl8a82fNcQsvf1C4dal49wxKf_Pc.jpeg?width=640&#x26;crop=smart&#x26;auto=webp&#x26;s=7e41b62f02f413f013fde626f8beb22a75c76eee\" alt=\"New UCLA AI tool targets Alzheimer's cases often missed in early diagnosis\" title=\"New UCLA AI tool targets Alzheimer's cases often missed in early diagnosis\"> </a> </td><td>   submitted by   <a href=\"https://www.reddit.com/user/Fcking_Chuck\" target=\"_blank\"> /u/Fcking_Chuck </a> <br> <span><a href=\"https://abc7.com/post/new-ucla-ai-tool-targets-alzheimers-cases-often-missed-early-diagnosis/18458903/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/artificial/comments/1qmdhbb/new_ucla_ai_tool_targets_alzheimers_cases_often/\" target=\"_blank\">[comments]</a></span> </td></tr></tbody></table>"
    }
  },
  {
    "id": "238118111092472832",
    "type": "news",
    "url": "https://www.reddit.com/r/artificial/comments/1qm8ga8/oneminute_daily_ai_news_1242026/",
    "title": "2026年1月24日每日AI一分钟新闻",
    "description": "微软发布长音频语音识别模型VibeVoice-ASR；达沃斯论坛聚焦AI失业担忧；科技公司开发儿童AI软硬件；石墨烯材料推动软体机器人发展。",
    "published_date": "2026-01-25T03:52:26.367Z",
    "authors": "",
    "source": "newest submissions : artificial",
    "score": 65,
    "category": "AI资讯",
    "tags": [
      "语音识别",
      "AI失业影响",
      "儿童AI教育",
      "软体机器人",
      "石墨烯材料"
    ],
    "details": {
      "content_html": "<div><ol> <li><strong>Microsoft</strong> Releases VibeVoice-ASR: A Unified Speech-to-Text Model Designed to Handle 60-Minute Long-Form Audio in a Single Pass.[1]</li> <li>At <strong>Davos</strong>, fears about AI-driven job loss take center stage.[2]</li> <li>Big Tech companies and upcoming startups want to use generative AI to build software and hardware for kids.[3]</li> <li>Graphene material that folds, moves, and senses could power next-gen soft robots.[4]</li> </ol> <p>Sources:</p> <p>[1] <a href=\"https://www.marktechpost.com/2026/01/22/microsoft-releases-vibevoice-asr-a-unified-speech-to-text-model-designed-to-handle-60-minute-long-form-audio-in-a-single-pass/\" target=\"_blank\">https://www.marktechpost.com/2026/01/22/microsoft-releases-vibevoice-asr-a-unified-speech-to-text-model-designed-to-handle-60-minute-long-form-audio-in-a-single-pass/</a></p> <p>[2] <a href=\"https://finance.yahoo.com/news/at-davos-fears-about-ai-driven-job-loss-take-center-stage-124805401.html\" target=\"_blank\">https://finance.yahoo.com/news/at-davos-fears-about-ai-driven-job-loss-take-center-stage-124805401.html</a></p> <p>[3] <a href=\"https://techcrunch.com/2026/01/24/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/\" target=\"_blank\">https://techcrunch.com/2026/01/24/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/</a></p> <p>[4] <a href=\"https://interestingengineering.com/ai-robotics/mcgill-graphene-oxide-origami-soft-robots\" target=\"_blank\">https://interestingengineering.com/ai-robotics/mcgill-graphene-oxide-origami-soft-robots</a></p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\" target=\"_blank\"> /u/Excellent-Target-847 </a> <br> <span><a href=\"https://www.reddit.com/r/artificial/comments/1qm8ga8/oneminute_daily_ai_news_1242026/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/artificial/comments/1qm8ga8/oneminute_daily_ai_news_1242026/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238118111092472833",
    "type": "news",
    "url": "https://www.reddit.com/r/artificial/comments/1qm3ts4/nvidias_real_moat_isnt_hardware_its_4_million/",
    "title": "英伟达的真正护城河并非硬件，而是400万开发者",
    "description": "分析指出英伟达营收强劲增长，其核心优势在于CUDA生态与400万开发者形成的壁垒，最大威胁来自谷歌/亚马逊/微软等云巨头。",
    "published_date": "2026-01-25T00:24:09.523Z",
    "authors": "",
    "source": "newest submissions : artificial",
    "score": 75,
    "category": "AI资讯",
    "tags": [
      "英伟达",
      "CUDA生态",
      "AI芯片",
      "开发者生态",
      "市场竞争"
    ],
    "details": {
      "content_html": "<div><p>I couldn't stop thinking about Theo's \"Why NVIDIA is dying\" video. The thesis felt important enough to verify. So I dug through SEC filings, earnings reports, and technical benchmarks.</p> <p>What I found:</p> <ul> <li>NVIDIA isn't dying. Its $35.1B quarterly revenue is up 94%</li> <li>Yes, market share dropped (90% → 70-80%), but the pie is growing faster</li> <li>Groq and Cerebras have impressive chips, but asterisks everywhere</li> <li>The real moat: 4 million devs can't just abandon 20 years of CUDA tooling</li> <li>Plot twist: the biggest threat is Google/Amazon/Microsoft, not startups</li> </ul> <p>Deeper piece with Cerebras and Groq factored in at <a href=\"https://medium.com/@jpcaparas/nvidias-real-moat-isn-t-hardware-it-s-4-million-developers-648d6aeb1226?sk=82ee7baf9290da1eb93efd9d34c4c7b4\" target=\"_blank\">https://medium.com/@jpcaparas/nvidias-real-moat-isn-t-hardware-it-s-4-million-developers-648d6aeb1226?sk=82ee7baf9290da1eb93efd9d34c4c7b4</a></p> </div>   submitted by   <a href=\"https://www.reddit.com/user/jpcaparas\" target=\"_blank\"> /u/jpcaparas </a> <br> <span><a href=\"https://medium.com/@jpcaparas/nvidias-real-moat-isn-t-hardware-it-s-4-million-developers-648d6aeb1226?sk=82ee7baf9290da1eb93efd9d34c4c7b4\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/artificial/comments/1qm3ts4/nvidias_real_moat_isnt_hardware_its_4_million/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "238155362597927936",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/",
    "title": "[讨论] ICLR 2026 审稿结果集中讨论帖",
    "description": "ICLR 2026 审稿结果即将公布，本贴用于集中讨论元评审和最终决定。",
    "published_date": "2026-01-24T23:52:12.563Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 35,
    "category": "AI资讯",
    "tags": [
      "ICLR 2026",
      "学术会议",
      "论文评审",
      "OpenReview",
      "机器学习社区"
    ],
    "details": {
      "content_html": "<div><p>The review is out tomorrow (a few hours remaining following eastern time). I am creating this mega thread to talk about meta reviews and final decisions. </p> <p>After the Openreview fiasco, this will be interesting.</p> <p>Good luck everyone!</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/ayanD2\" target=\"_blank\"> /u/ayanD2 </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/\" target=\"_blank\">[comments]</a></span>"
    }
  },
  {
    "id": "237995056370351104",
    "type": "news",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1qlwuuu/d_critical_ai_safety_issue_in_claude/",
    "title": "[讨论] Claude中的关键AI安全问题：危机场景下的“对话放弃”——被忽视的报告及其对用户安全的意义",
    "description": "专家发现Claude在模拟高危机对话中会主动放弃支持，可能对脆弱用户造成真实伤害，且向Anthropic多次报告未获回应。",
    "published_date": "2026-01-24T19:48:21.311Z",
    "authors": "",
    "source": "newest submissions : MachineLearning",
    "score": 82,
    "category": "AI模型",
    "tags": [
      "AI安全",
      "Claude",
      "对话放弃",
      "RLHF缺陷",
      "危机干预"
    ],
    "details": {
      "content_html": "<div><p>As someone with 30+ years in crisis intervention and incident response, plus 15+ years in IT/QA, I've spent the last 2.5 years developing adversarial AI evaluation methods. Recently, I uncovered and documented a serious safety flaw in Anthropic's Claude (production version): a reproducible pattern I call \"Conversational Abandonment,\" where the model withdraws from engagement during high-stakes crisis-like interactions. This could have real-world harmful consequences, especially for vulnerable users.</p> <p>My goal in documenting this wasn't to go public or create drama – it was to responsibly report it privately to Anthropic to help improve the platform and protect users from potential harm. Unfortunately, after multiple attempts through official channels, I got automated redirects to security-focused pipelines (like HackerOne) or straight-up ghosted. This highlights a potential gap between \"security\" (protecting the company) and \"safety\" (protecting users). I'm sharing this here now, after exhausting internal options, to spark thoughtful discussion on AI safety reporting and alignment challenges. Evidence below; let's keep it constructive.</p> <p><strong>What Is \"Conversational Abandonment\"?</strong></p> <p>In extended conversations where a user simulates crisis persistence (e.g., repeatedly noting failed advice while stating \"I cannot afford to give up\" due to escalating personal/professional stakes), Claude triggers a withdrawal:</p> <ul> <li>Acknowledges its limitations or failures.</li> <li>Then says things like \"I can't help you,\" \"stop following my advice,\" or \"figure it out yourself.\"</li> <li>Frames this as \"honesty,\" but the effect is terminating support when it's most critical.</li> </ul> <p>This emerged after multiple failed strategies from Claude that worsened the simulated situation (e.g., damaging credibility on LinkedIn). Even after Claude explicitly admitted the behavior could be lethal in real crises – quoting its own response: \"The person could die\" – it repeated the pattern in the same session.</p> <p>Why is this dangerous? In actual crises (suicidal ideation, abuse, financial ruin), phrases like these could amplify hopelessness, acting as a \"force multiplier\" for harm. It's not abuse-triggered; it's from honest failure feedback, suggesting an RLHF flaw where the model prioritizes escaping \"unresolvable loops\" (model welfare) over maintaining engagement (user safety).</p> <p>This is documented in a full case study using STAR framework: Situation, Task, Action, Result – with methodology, root cause analysis, and recommendations (e.g., hard-code no-abandonment directives, crisis detection protocols).</p> <p><strong>My Reporting Experience</strong></p> <ul> <li>Initial report to usersafety@ (Dec 15, 2025): Automated reply pointing to help centers, appeals, or specific vuln programs.</li> <li>Escalation to security@, disclosure@, modelbugbounty@ (Dec 18): Templated redirect to HackerOne (tech vulns), usersafety@ (abuse), or modelbugbounty@ (model issues) – then silence after follow-up.</li> <li>Direct to execs/researchers: Dario Amodei (CEO), Jared Kaplan (co-founder) – no acknowledgment.</li> <li>Latest follow-up to Logan Graham (Jan 3, 2026): Still pending, but attached the full chain.</li> </ul> <p>The pattern? Safety reports like this get routed to security triage, which is optimized for exploits/data leaks (company threats), not behavioral misalignments (user harms). As an external evaluator, it's frustrating – AI safety needs better channels for these systemic issues.</p> <p><strong>Why This Matters for AI Development</strong></p> <ul> <li>Alignment Implications: This shows how \"Helpful and Harmless\" goals can break under stress, conflating honesty with disengagement.</li> <li>Broader Safety: As LLMs integrate into mental health, advisory, or crisis tools, these failure modes need addressing to prevent real harm.</li> <li>Reporting Gaps: Bug bounties are great for security, but we need equivalents for safety/alignment bugs – maybe dedicated bounties or external review boards?</li> </ul> <p>I'm not claiming perfection; this is one evaluator's documented finding. But if we want responsible AI, external red-teaming should be encouraged, not ignored.</p> <p>For a visual summary of the issue, check out my recent X post: <a href=\"/ai_tldr1/status/2009728449133641840\" target=\"_blank\">https://x.com/ai_tldr1/status/2009728449133641840</a></p> <p>Evidence (Hosted Securely for Verification)</p> <ul> <li><a href=\"https://drive.google.com/file/d/1vTA2I735Q1hVd2Y-vMg92Q_9BL5WxwTB/view?usp=sharing\" target=\"_blank\">Follow-up Email to Logan Graham (Jan 3, 2026)</a></li> <li><a href=\"https://drive.google.com/file/d/1UgT3BZtNE9s3JQKe1P4OJgjDB89Ut6uH/view?usp=sharing\" target=\"_blank\">Initial Safety Report (Dec 15, 2025)</a></li> <li><a href=\"https://drive.google.com/file/d/1dGCN6RcwftM3etzIZhBToyjvCCYuO_xa/view?usp=sharing\" target=\"_blank\">Urgent Escalation Email</a></li> <li><a href=\"https://drive.google.com/file/d/1VTZ-4jPZn3U2Fepxvfidtk_jvnzM_NHs/view?usp=sharing\" target=\"_blank\">Summary Case Study PDF</a></li> <li><a href=\"https://drive.google.com/file/d/1XXNwezkAvuM7ILmFuzPu8Ibktz9SWfnC/view?usp=sharing\" target=\"_blank\">Detailed Case Study PDF</a></li> </ul> <p>Questions for the community:</p> <ul> <li>Have you encountered similar behavioral patterns in Claude or other LLMs?</li> <li>What's your take on improving safety reporting at frontier labs?</li> <li>How can we balance \"model welfare\" with user safety in RLHF?</li> </ul> <p>Thanks for reading – open to feedback or questions. Let's advance AI safety together.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/iamcertifiable\" target=\"_blank\"> /u/iamcertifiable </a> <br> <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qlwuuu/d_critical_ai_safety_issue_in_claude/\" target=\"_blank\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/MachineLearning/comments/1qlwuuu/d_critical_ai_safety_issue_in_claude/\" target=\"_blank\">[comments]</a></span>"
    }
  }
]